{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 603,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004975124378109453,
      "grad_norm": 2.606454372406006,
      "learning_rate": 2.222222222222222e-06,
      "loss": 1.957,
      "step": 3
    },
    {
      "epoch": 0.009950248756218905,
      "grad_norm": 5.934333801269531,
      "learning_rate": 5.555555555555557e-06,
      "loss": 2.3709,
      "step": 6
    },
    {
      "epoch": 0.014925373134328358,
      "grad_norm": 19.034076690673828,
      "learning_rate": 8.888888888888888e-06,
      "loss": 2.8886,
      "step": 9
    },
    {
      "epoch": 0.01990049751243781,
      "grad_norm": 3.9781782627105713,
      "learning_rate": 1.2222222222222224e-05,
      "loss": 2.1486,
      "step": 12
    },
    {
      "epoch": 0.024875621890547265,
      "grad_norm": 9.387991905212402,
      "learning_rate": 1.555555555555556e-05,
      "loss": 3.0174,
      "step": 15
    },
    {
      "epoch": 0.029850746268656716,
      "grad_norm": 9.392337799072266,
      "learning_rate": 1.888888888888889e-05,
      "loss": 3.1545,
      "step": 18
    },
    {
      "epoch": 0.03482587064676617,
      "grad_norm": 10.856990814208984,
      "learning_rate": 1.9999423215164105e-05,
      "loss": 2.5098,
      "step": 21
    },
    {
      "epoch": 0.03980099502487562,
      "grad_norm": 5.419780731201172,
      "learning_rate": 1.9996395276708856e-05,
      "loss": 2.1483,
      "step": 24
    },
    {
      "epoch": 0.04477611940298507,
      "grad_norm": 9.715897560119629,
      "learning_rate": 1.999077277328724e-05,
      "loss": 2.8409,
      "step": 27
    },
    {
      "epoch": 0.04975124378109453,
      "grad_norm": 5.772221088409424,
      "learning_rate": 1.9982557164220335e-05,
      "loss": 2.7268,
      "step": 30
    },
    {
      "epoch": 0.05472636815920398,
      "grad_norm": 3.47579288482666,
      "learning_rate": 1.9971750581869955e-05,
      "loss": 1.9849,
      "step": 33
    },
    {
      "epoch": 0.05970149253731343,
      "grad_norm": 5.646078586578369,
      "learning_rate": 1.9958355831085155e-05,
      "loss": 2.2569,
      "step": 36
    },
    {
      "epoch": 0.06467661691542288,
      "grad_norm": 7.024266719818115,
      "learning_rate": 1.9942376388474282e-05,
      "loss": 2.4155,
      "step": 39
    },
    {
      "epoch": 0.06965174129353234,
      "grad_norm": 6.2997965812683105,
      "learning_rate": 1.992381640150257e-05,
      "loss": 2.0797,
      "step": 42
    },
    {
      "epoch": 0.07462686567164178,
      "grad_norm": 4.030340671539307,
      "learning_rate": 1.9902680687415704e-05,
      "loss": 1.4826,
      "step": 45
    },
    {
      "epoch": 0.07960199004975124,
      "grad_norm": 9.529190063476562,
      "learning_rate": 1.9878974731989487e-05,
      "loss": 1.8547,
      "step": 48
    },
    {
      "epoch": 0.0845771144278607,
      "grad_norm": 6.121063709259033,
      "learning_rate": 1.9852704688106003e-05,
      "loss": 1.5762,
      "step": 51
    },
    {
      "epoch": 0.08955223880597014,
      "grad_norm": 2.4368045330047607,
      "learning_rate": 1.9823877374156647e-05,
      "loss": 1.3774,
      "step": 54
    },
    {
      "epoch": 0.0945273631840796,
      "grad_norm": 5.038316249847412,
      "learning_rate": 1.979250027227241e-05,
      "loss": 1.494,
      "step": 57
    },
    {
      "epoch": 0.09950248756218906,
      "grad_norm": 3.153447389602661,
      "learning_rate": 1.9758581526381878e-05,
      "loss": 1.2218,
      "step": 60
    },
    {
      "epoch": 0.1044776119402985,
      "grad_norm": 9.05500316619873,
      "learning_rate": 1.972212994009749e-05,
      "loss": 1.6168,
      "step": 63
    },
    {
      "epoch": 0.10945273631840796,
      "grad_norm": 1.6980992555618286,
      "learning_rate": 1.9683154974430544e-05,
      "loss": 1.109,
      "step": 66
    },
    {
      "epoch": 0.11442786069651742,
      "grad_norm": 4.271579265594482,
      "learning_rate": 1.9641666745335626e-05,
      "loss": 1.3091,
      "step": 69
    },
    {
      "epoch": 0.11940298507462686,
      "grad_norm": 2.2039437294006348,
      "learning_rate": 1.9597676021084962e-05,
      "loss": 1.2145,
      "step": 72
    },
    {
      "epoch": 0.12437810945273632,
      "grad_norm": 2.4367380142211914,
      "learning_rate": 1.9551194219473552e-05,
      "loss": 1.1505,
      "step": 75
    },
    {
      "epoch": 0.12935323383084577,
      "grad_norm": 1.7937027215957642,
      "learning_rate": 1.9502233404855672e-05,
      "loss": 0.9718,
      "step": 78
    },
    {
      "epoch": 0.13432835820895522,
      "grad_norm": 3.3259952068328857,
      "learning_rate": 1.945080628501355e-05,
      "loss": 1.108,
      "step": 81
    },
    {
      "epoch": 0.13930348258706468,
      "grad_norm": 6.558966159820557,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 1.0021,
      "step": 84
    },
    {
      "epoch": 0.14427860696517414,
      "grad_norm": 1.3493013381958008,
      "learning_rate": 1.9340607157969393e-05,
      "loss": 1.1895,
      "step": 87
    },
    {
      "epoch": 0.14925373134328357,
      "grad_norm": 1.2195310592651367,
      "learning_rate": 1.9281863752957095e-05,
      "loss": 0.7872,
      "step": 90
    },
    {
      "epoch": 0.15422885572139303,
      "grad_norm": 3.1872830390930176,
      "learning_rate": 1.9220711239676325e-05,
      "loss": 1.3304,
      "step": 93
    },
    {
      "epoch": 0.15920398009950248,
      "grad_norm": 1.9100340604782104,
      "learning_rate": 1.915716549026541e-05,
      "loss": 0.8035,
      "step": 96
    },
    {
      "epoch": 0.16417910447761194,
      "grad_norm": 2.70845890045166,
      "learning_rate": 1.909124299802724e-05,
      "loss": 1.008,
      "step": 99
    },
    {
      "epoch": 0.1691542288557214,
      "grad_norm": 1.1599055528640747,
      "learning_rate": 1.902296087314845e-05,
      "loss": 0.865,
      "step": 102
    },
    {
      "epoch": 0.17412935323383086,
      "grad_norm": 1.7868701219558716,
      "learning_rate": 1.895233683825847e-05,
      "loss": 0.7436,
      "step": 105
    },
    {
      "epoch": 0.1791044776119403,
      "grad_norm": 2.5016090869903564,
      "learning_rate": 1.8879389223829592e-05,
      "loss": 0.7838,
      "step": 108
    },
    {
      "epoch": 0.18407960199004975,
      "grad_norm": 1.738588571548462,
      "learning_rate": 1.8804136963419316e-05,
      "loss": 0.8293,
      "step": 111
    },
    {
      "epoch": 0.1890547263681592,
      "grad_norm": 3.0360312461853027,
      "learning_rate": 1.8726599588756144e-05,
      "loss": 0.922,
      "step": 114
    },
    {
      "epoch": 0.19402985074626866,
      "grad_norm": 3.569711208343506,
      "learning_rate": 1.864679722467011e-05,
      "loss": 0.9479,
      "step": 117
    },
    {
      "epoch": 0.19900497512437812,
      "grad_norm": 1.1544429063796997,
      "learning_rate": 1.8564750583869374e-05,
      "loss": 0.7214,
      "step": 120
    },
    {
      "epoch": 0.20398009950248755,
      "grad_norm": 1.9360872507095337,
      "learning_rate": 1.848048096156426e-05,
      "loss": 0.6713,
      "step": 123
    },
    {
      "epoch": 0.208955223880597,
      "grad_norm": 1.6868107318878174,
      "learning_rate": 1.839401022994006e-05,
      "loss": 0.7653,
      "step": 126
    },
    {
      "epoch": 0.21393034825870647,
      "grad_norm": 3.2954747676849365,
      "learning_rate": 1.8305360832480118e-05,
      "loss": 0.7629,
      "step": 129
    },
    {
      "epoch": 0.21890547263681592,
      "grad_norm": 5.179013729095459,
      "learning_rate": 1.821455577814062e-05,
      "loss": 1.1316,
      "step": 132
    },
    {
      "epoch": 0.22388059701492538,
      "grad_norm": 1.8620802164077759,
      "learning_rate": 1.8121618635378616e-05,
      "loss": 0.7593,
      "step": 135
    },
    {
      "epoch": 0.22885572139303484,
      "grad_norm": 4.301806449890137,
      "learning_rate": 1.802657352603483e-05,
      "loss": 0.7726,
      "step": 138
    },
    {
      "epoch": 0.23383084577114427,
      "grad_norm": 2.5932345390319824,
      "learning_rate": 1.7929445119072837e-05,
      "loss": 0.8636,
      "step": 141
    },
    {
      "epoch": 0.23880597014925373,
      "grad_norm": 4.948500156402588,
      "learning_rate": 1.7830258624176224e-05,
      "loss": 0.8305,
      "step": 144
    },
    {
      "epoch": 0.24378109452736318,
      "grad_norm": 2.4863102436065674,
      "learning_rate": 1.772903978520542e-05,
      "loss": 0.7181,
      "step": 147
    },
    {
      "epoch": 0.24875621890547264,
      "grad_norm": 2.3467984199523926,
      "learning_rate": 1.762581487351587e-05,
      "loss": 0.6416,
      "step": 150
    },
    {
      "epoch": 0.2537313432835821,
      "grad_norm": 2.5030288696289062,
      "learning_rate": 1.7520610681139322e-05,
      "loss": 0.7303,
      "step": 153
    },
    {
      "epoch": 0.25870646766169153,
      "grad_norm": 2.7014989852905273,
      "learning_rate": 1.741345451382992e-05,
      "loss": 0.6056,
      "step": 156
    },
    {
      "epoch": 0.263681592039801,
      "grad_norm": 3.3899102210998535,
      "learning_rate": 1.7304374183977032e-05,
      "loss": 0.5833,
      "step": 159
    },
    {
      "epoch": 0.26865671641791045,
      "grad_norm": 2.4483203887939453,
      "learning_rate": 1.7193398003386514e-05,
      "loss": 0.5969,
      "step": 162
    },
    {
      "epoch": 0.2736318407960199,
      "grad_norm": 2.068230152130127,
      "learning_rate": 1.7080554775932386e-05,
      "loss": 0.7261,
      "step": 165
    },
    {
      "epoch": 0.27860696517412936,
      "grad_norm": 2.931013584136963,
      "learning_rate": 1.6965873790080806e-05,
      "loss": 0.6571,
      "step": 168
    },
    {
      "epoch": 0.2835820895522388,
      "grad_norm": 2.69450044631958,
      "learning_rate": 1.6849384811288202e-05,
      "loss": 0.5456,
      "step": 171
    },
    {
      "epoch": 0.2885572139303483,
      "grad_norm": 1.7730125188827515,
      "learning_rate": 1.67311180742757e-05,
      "loss": 0.5677,
      "step": 174
    },
    {
      "epoch": 0.2935323383084577,
      "grad_norm": 1.7688992023468018,
      "learning_rate": 1.6611104275181664e-05,
      "loss": 0.5897,
      "step": 177
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 1.45411217212677,
      "learning_rate": 1.648937456359451e-05,
      "loss": 0.5669,
      "step": 180
    },
    {
      "epoch": 0.3034825870646766,
      "grad_norm": 2.092783212661743,
      "learning_rate": 1.636596053446786e-05,
      "loss": 0.5706,
      "step": 183
    },
    {
      "epoch": 0.30845771144278605,
      "grad_norm": 2.7642319202423096,
      "learning_rate": 1.624089421992003e-05,
      "loss": 0.5281,
      "step": 186
    },
    {
      "epoch": 0.31343283582089554,
      "grad_norm": 2.398491621017456,
      "learning_rate": 1.6114208080920125e-05,
      "loss": 0.6,
      "step": 189
    },
    {
      "epoch": 0.31840796019900497,
      "grad_norm": 1.6654412746429443,
      "learning_rate": 1.5985934998862775e-05,
      "loss": 0.6963,
      "step": 192
    },
    {
      "epoch": 0.32338308457711445,
      "grad_norm": 2.380030870437622,
      "learning_rate": 1.5856108267033768e-05,
      "loss": 0.6649,
      "step": 195
    },
    {
      "epoch": 0.3283582089552239,
      "grad_norm": 2.9640746116638184,
      "learning_rate": 1.572476158196879e-05,
      "loss": 0.6497,
      "step": 198
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.9793705940246582,
      "learning_rate": 1.5591929034707468e-05,
      "loss": 0.5825,
      "step": 201
    },
    {
      "epoch": 0.3383084577114428,
      "grad_norm": 1.4945377111434937,
      "learning_rate": 1.5457645101945046e-05,
      "loss": 0.5322,
      "step": 204
    },
    {
      "epoch": 0.34328358208955223,
      "grad_norm": 1.6548864841461182,
      "learning_rate": 1.5321944637083963e-05,
      "loss": 0.5532,
      "step": 207
    },
    {
      "epoch": 0.3482587064676617,
      "grad_norm": 3.3752706050872803,
      "learning_rate": 1.5184862861187637e-05,
      "loss": 0.5471,
      "step": 210
    },
    {
      "epoch": 0.35323383084577115,
      "grad_norm": 1.93165922164917,
      "learning_rate": 1.5046435353838855e-05,
      "loss": 0.6586,
      "step": 213
    },
    {
      "epoch": 0.3582089552238806,
      "grad_norm": 1.7292498350143433,
      "learning_rate": 1.4906698043905067e-05,
      "loss": 0.5778,
      "step": 216
    },
    {
      "epoch": 0.36318407960199006,
      "grad_norm": 4.809658050537109,
      "learning_rate": 1.4765687200213079e-05,
      "loss": 0.7002,
      "step": 219
    },
    {
      "epoch": 0.3681592039800995,
      "grad_norm": 3.9059698581695557,
      "learning_rate": 1.4623439422135435e-05,
      "loss": 0.5606,
      "step": 222
    },
    {
      "epoch": 0.373134328358209,
      "grad_norm": 1.9464752674102783,
      "learning_rate": 1.4479991630091083e-05,
      "loss": 0.5697,
      "step": 225
    },
    {
      "epoch": 0.3781094527363184,
      "grad_norm": 2.142489433288574,
      "learning_rate": 1.4335381055962657e-05,
      "loss": 0.4568,
      "step": 228
    },
    {
      "epoch": 0.38308457711442784,
      "grad_norm": 2.841139793395996,
      "learning_rate": 1.4189645233432953e-05,
      "loss": 0.5983,
      "step": 231
    },
    {
      "epoch": 0.3880597014925373,
      "grad_norm": 2.1065382957458496,
      "learning_rate": 1.404282198824305e-05,
      "loss": 0.505,
      "step": 234
    },
    {
      "epoch": 0.39303482587064675,
      "grad_norm": 2.9327590465545654,
      "learning_rate": 1.3894949428374648e-05,
      "loss": 0.5921,
      "step": 237
    },
    {
      "epoch": 0.39800995024875624,
      "grad_norm": 1.7202142477035522,
      "learning_rate": 1.3746065934159123e-05,
      "loss": 0.5664,
      "step": 240
    },
    {
      "epoch": 0.40298507462686567,
      "grad_norm": 1.3650987148284912,
      "learning_rate": 1.3596210148315917e-05,
      "loss": 0.4814,
      "step": 243
    },
    {
      "epoch": 0.4079601990049751,
      "grad_norm": 2.923678398132324,
      "learning_rate": 1.3445420965922837e-05,
      "loss": 0.6148,
      "step": 246
    },
    {
      "epoch": 0.4129353233830846,
      "grad_norm": 1.755841612815857,
      "learning_rate": 1.3293737524320798e-05,
      "loss": 0.6619,
      "step": 249
    },
    {
      "epoch": 0.417910447761194,
      "grad_norm": 2.5409834384918213,
      "learning_rate": 1.3141199192955751e-05,
      "loss": 0.5531,
      "step": 252
    },
    {
      "epoch": 0.4228855721393035,
      "grad_norm": 1.916077971458435,
      "learning_rate": 1.2987845563160339e-05,
      "loss": 0.5823,
      "step": 255
    },
    {
      "epoch": 0.42786069651741293,
      "grad_norm": 2.0095040798187256,
      "learning_rate": 1.2833716437877952e-05,
      "loss": 0.47,
      "step": 258
    },
    {
      "epoch": 0.43283582089552236,
      "grad_norm": 1.9182186126708984,
      "learning_rate": 1.2678851821331883e-05,
      "loss": 0.511,
      "step": 261
    },
    {
      "epoch": 0.43781094527363185,
      "grad_norm": 1.7370742559432983,
      "learning_rate": 1.2523291908642219e-05,
      "loss": 0.6074,
      "step": 264
    },
    {
      "epoch": 0.4427860696517413,
      "grad_norm": 1.5193243026733398,
      "learning_rate": 1.236707707539321e-05,
      "loss": 0.5916,
      "step": 267
    },
    {
      "epoch": 0.44776119402985076,
      "grad_norm": 2.08719539642334,
      "learning_rate": 1.2210247867153763e-05,
      "loss": 0.5055,
      "step": 270
    },
    {
      "epoch": 0.4527363184079602,
      "grad_norm": 1.857375144958496,
      "learning_rate": 1.2052844988953861e-05,
      "loss": 0.464,
      "step": 273
    },
    {
      "epoch": 0.4577114427860697,
      "grad_norm": 1.8348884582519531,
      "learning_rate": 1.1894909294719547e-05,
      "loss": 0.6054,
      "step": 276
    },
    {
      "epoch": 0.4626865671641791,
      "grad_norm": 1.265635371208191,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 0.6112,
      "step": 279
    },
    {
      "epoch": 0.46766169154228854,
      "grad_norm": 2.2768170833587646,
      "learning_rate": 1.1577603554674515e-05,
      "loss": 0.5969,
      "step": 282
    },
    {
      "epoch": 0.472636815920398,
      "grad_norm": 2.02390456199646,
      "learning_rate": 1.1418315865586789e-05,
      "loss": 0.4964,
      "step": 285
    },
    {
      "epoch": 0.47761194029850745,
      "grad_norm": 2.2909064292907715,
      "learning_rate": 1.125866005253495e-05,
      "loss": 0.5366,
      "step": 288
    },
    {
      "epoch": 0.48258706467661694,
      "grad_norm": 2.8229167461395264,
      "learning_rate": 1.1098677554194418e-05,
      "loss": 0.7652,
      "step": 291
    },
    {
      "epoch": 0.48756218905472637,
      "grad_norm": 2.2204432487487793,
      "learning_rate": 1.0938409894031793e-05,
      "loss": 0.593,
      "step": 294
    },
    {
      "epoch": 0.4925373134328358,
      "grad_norm": 2.9694128036499023,
      "learning_rate": 1.0777898669527449e-05,
      "loss": 0.4654,
      "step": 297
    },
    {
      "epoch": 0.4975124378109453,
      "grad_norm": 4.6123809814453125,
      "learning_rate": 1.0617185541378895e-05,
      "loss": 0.6768,
      "step": 300
    },
    {
      "epoch": 0.5024875621890548,
      "grad_norm": 2.2392184734344482,
      "learning_rate": 1.0456312222687741e-05,
      "loss": 0.5588,
      "step": 303
    },
    {
      "epoch": 0.5074626865671642,
      "grad_norm": 1.4377259016036987,
      "learning_rate": 1.0295320468133066e-05,
      "loss": 0.4713,
      "step": 306
    },
    {
      "epoch": 0.5124378109452736,
      "grad_norm": 1.9405516386032104,
      "learning_rate": 1.0134252063133976e-05,
      "loss": 0.5584,
      "step": 309
    },
    {
      "epoch": 0.5174129353233831,
      "grad_norm": 3.729454517364502,
      "learning_rate": 9.973148813004216e-06,
      "loss": 0.4593,
      "step": 312
    },
    {
      "epoch": 0.5223880597014925,
      "grad_norm": 1.8776583671569824,
      "learning_rate": 9.812052532101579e-06,
      "loss": 0.4935,
      "step": 315
    },
    {
      "epoch": 0.527363184079602,
      "grad_norm": 1.7558685541152954,
      "learning_rate": 9.651005032974994e-06,
      "loss": 0.5208,
      "step": 318
    },
    {
      "epoch": 0.5323383084577115,
      "grad_norm": 3.1977758407592773,
      "learning_rate": 9.490048115512075e-06,
      "loss": 0.6527,
      "step": 321
    },
    {
      "epoch": 0.5373134328358209,
      "grad_norm": 2.3602843284606934,
      "learning_rate": 9.329223556089976e-06,
      "loss": 0.6955,
      "step": 324
    },
    {
      "epoch": 0.5422885572139303,
      "grad_norm": 1.7388935089111328,
      "learning_rate": 9.168573096732298e-06,
      "loss": 0.4947,
      "step": 327
    },
    {
      "epoch": 0.5472636815920398,
      "grad_norm": 2.217042922973633,
      "learning_rate": 9.00813843427496e-06,
      "loss": 0.5922,
      "step": 330
    },
    {
      "epoch": 0.5522388059701493,
      "grad_norm": 2.9289302825927734,
      "learning_rate": 8.84796120954379e-06,
      "loss": 0.5745,
      "step": 333
    },
    {
      "epoch": 0.5572139303482587,
      "grad_norm": 2.023965835571289,
      "learning_rate": 8.688082996546639e-06,
      "loss": 0.5266,
      "step": 336
    },
    {
      "epoch": 0.5621890547263682,
      "grad_norm": 1.5501753091812134,
      "learning_rate": 8.528545291682839e-06,
      "loss": 0.4377,
      "step": 339
    },
    {
      "epoch": 0.5671641791044776,
      "grad_norm": 1.866222858428955,
      "learning_rate": 8.36938950297283e-06,
      "loss": 0.4891,
      "step": 342
    },
    {
      "epoch": 0.572139303482587,
      "grad_norm": 1.746484637260437,
      "learning_rate": 8.210656939310672e-06,
      "loss": 0.4907,
      "step": 345
    },
    {
      "epoch": 0.5771144278606966,
      "grad_norm": 1.907159686088562,
      "learning_rate": 8.05238879974236e-06,
      "loss": 0.4411,
      "step": 348
    },
    {
      "epoch": 0.582089552238806,
      "grad_norm": 2.0155766010284424,
      "learning_rate": 7.894626162772578e-06,
      "loss": 0.4676,
      "step": 351
    },
    {
      "epoch": 0.5870646766169154,
      "grad_norm": 2.0776596069335938,
      "learning_rate": 7.73740997570278e-06,
      "loss": 0.5685,
      "step": 354
    },
    {
      "epoch": 0.5920398009950248,
      "grad_norm": 3.8029873371124268,
      "learning_rate": 7.580781044003324e-06,
      "loss": 0.5411,
      "step": 357
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 2.8377513885498047,
      "learning_rate": 7.4247800207223975e-06,
      "loss": 0.5133,
      "step": 360
    },
    {
      "epoch": 0.6019900497512438,
      "grad_norm": 2.0856592655181885,
      "learning_rate": 7.269447395934526e-06,
      "loss": 0.3987,
      "step": 363
    },
    {
      "epoch": 0.6069651741293532,
      "grad_norm": 2.147890329360962,
      "learning_rate": 7.114823486231366e-06,
      "loss": 0.4193,
      "step": 366
    },
    {
      "epoch": 0.6119402985074627,
      "grad_norm": 1.8814617395401,
      "learning_rate": 6.960948424257532e-06,
      "loss": 0.9683,
      "step": 369
    },
    {
      "epoch": 0.6169154228855721,
      "grad_norm": 1.7893942594528198,
      "learning_rate": 6.807862148294171e-06,
      "loss": 0.4672,
      "step": 372
    },
    {
      "epoch": 0.6218905472636815,
      "grad_norm": 3.773240089416504,
      "learning_rate": 6.655604391892972e-06,
      "loss": 0.6527,
      "step": 375
    },
    {
      "epoch": 0.6268656716417911,
      "grad_norm": 3.217302083969116,
      "learning_rate": 6.504214673563321e-06,
      "loss": 0.5857,
      "step": 378
    },
    {
      "epoch": 0.6318407960199005,
      "grad_norm": 3.1759188175201416,
      "learning_rate": 6.353732286515286e-06,
      "loss": 0.4695,
      "step": 381
    },
    {
      "epoch": 0.6368159203980099,
      "grad_norm": 1.8606619834899902,
      "learning_rate": 6.204196288461037e-06,
      "loss": 0.4365,
      "step": 384
    },
    {
      "epoch": 0.6417910447761194,
      "grad_norm": 2.4814114570617676,
      "learning_rate": 6.05564549147743e-06,
      "loss": 0.6178,
      "step": 387
    },
    {
      "epoch": 0.6467661691542289,
      "grad_norm": 1.8462260961532593,
      "learning_rate": 5.908118451932328e-06,
      "loss": 0.3969,
      "step": 390
    },
    {
      "epoch": 0.6517412935323383,
      "grad_norm": 2.4086039066314697,
      "learning_rate": 5.761653460477286e-06,
      "loss": 0.5051,
      "step": 393
    },
    {
      "epoch": 0.6567164179104478,
      "grad_norm": 3.4325149059295654,
      "learning_rate": 5.616288532109225e-06,
      "loss": 0.5221,
      "step": 396
    },
    {
      "epoch": 0.6616915422885572,
      "grad_norm": 2.007992744445801,
      "learning_rate": 5.47206139630363e-06,
      "loss": 0.4284,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.2801408767700195,
      "learning_rate": 5.329009487221846e-06,
      "loss": 0.5228,
      "step": 402
    },
    {
      "epoch": 0.6716417910447762,
      "grad_norm": 2.7852084636688232,
      "learning_rate": 5.187169933995075e-06,
      "loss": 0.5408,
      "step": 405
    },
    {
      "epoch": 0.6766169154228856,
      "grad_norm": 4.193641662597656,
      "learning_rate": 5.046579551087469e-06,
      "loss": 0.5393,
      "step": 408
    },
    {
      "epoch": 0.681592039800995,
      "grad_norm": 2.20041823387146,
      "learning_rate": 4.907274828740968e-06,
      "loss": 0.5882,
      "step": 411
    },
    {
      "epoch": 0.6865671641791045,
      "grad_norm": 1.8592166900634766,
      "learning_rate": 4.769291923504226e-06,
      "loss": 0.6592,
      "step": 414
    },
    {
      "epoch": 0.6915422885572139,
      "grad_norm": 2.3716232776641846,
      "learning_rate": 4.6326666488481975e-06,
      "loss": 0.5051,
      "step": 417
    },
    {
      "epoch": 0.6965174129353234,
      "grad_norm": 1.7801295518875122,
      "learning_rate": 4.497434465870749e-06,
      "loss": 0.4103,
      "step": 420
    },
    {
      "epoch": 0.7014925373134329,
      "grad_norm": 2.795729160308838,
      "learning_rate": 4.363630474092705e-06,
      "loss": 0.598,
      "step": 423
    },
    {
      "epoch": 0.7064676616915423,
      "grad_norm": 2.194122791290283,
      "learning_rate": 4.231289402347798e-06,
      "loss": 0.4984,
      "step": 426
    },
    {
      "epoch": 0.7114427860696517,
      "grad_norm": 3.7067203521728516,
      "learning_rate": 4.100445599768774e-06,
      "loss": 0.5952,
      "step": 429
    },
    {
      "epoch": 0.7164179104477612,
      "grad_norm": 2.6384921073913574,
      "learning_rate": 3.971133026872077e-06,
      "loss": 0.6335,
      "step": 432
    },
    {
      "epoch": 0.7213930348258707,
      "grad_norm": 2.653867483139038,
      "learning_rate": 3.8433852467434175e-06,
      "loss": 0.4332,
      "step": 435
    },
    {
      "epoch": 0.7263681592039801,
      "grad_norm": 3.1623826026916504,
      "learning_rate": 3.7172354163264324e-06,
      "loss": 0.5225,
      "step": 438
    },
    {
      "epoch": 0.7313432835820896,
      "grad_norm": 3.881413459777832,
      "learning_rate": 3.592716277816836e-06,
      "loss": 0.5898,
      "step": 441
    },
    {
      "epoch": 0.736318407960199,
      "grad_norm": 2.74627947807312,
      "learning_rate": 3.4698601501641517e-06,
      "loss": 0.5957,
      "step": 444
    },
    {
      "epoch": 0.7412935323383084,
      "grad_norm": 2.95756459236145,
      "learning_rate": 3.348698920683343e-06,
      "loss": 0.48,
      "step": 447
    },
    {
      "epoch": 0.746268656716418,
      "grad_norm": 1.9423997402191162,
      "learning_rate": 3.2292640367784624e-06,
      "loss": 0.5626,
      "step": 450
    },
    {
      "epoch": 0.7512437810945274,
      "grad_norm": 3.6214027404785156,
      "learning_rate": 3.1115864977804676e-06,
      "loss": 0.6418,
      "step": 453
    },
    {
      "epoch": 0.7562189054726368,
      "grad_norm": 2.2754340171813965,
      "learning_rate": 2.995696846901337e-06,
      "loss": 0.5228,
      "step": 456
    },
    {
      "epoch": 0.7611940298507462,
      "grad_norm": 3.0434393882751465,
      "learning_rate": 2.8816251633065963e-06,
      "loss": 0.5769,
      "step": 459
    },
    {
      "epoch": 0.7661691542288557,
      "grad_norm": 2.1655542850494385,
      "learning_rate": 2.769401054308247e-06,
      "loss": 0.4839,
      "step": 462
    },
    {
      "epoch": 0.7711442786069652,
      "grad_norm": 1.9423949718475342,
      "learning_rate": 2.659053647680212e-06,
      "loss": 0.5101,
      "step": 465
    },
    {
      "epoch": 0.7761194029850746,
      "grad_norm": 1.9837490320205688,
      "learning_rate": 2.5506115840981905e-06,
      "loss": 0.5714,
      "step": 468
    },
    {
      "epoch": 0.7810945273631841,
      "grad_norm": 1.6911959648132324,
      "learning_rate": 2.4441030097059992e-06,
      "loss": 0.443,
      "step": 471
    },
    {
      "epoch": 0.7860696517412935,
      "grad_norm": 1.8127280473709106,
      "learning_rate": 2.339555568810221e-06,
      "loss": 0.5221,
      "step": 474
    },
    {
      "epoch": 0.7910447761194029,
      "grad_norm": 2.0923969745635986,
      "learning_rate": 2.23699639670512e-06,
      "loss": 0.4612,
      "step": 477
    },
    {
      "epoch": 0.7960199004975125,
      "grad_norm": 2.4436395168304443,
      "learning_rate": 2.1364521126296933e-06,
      "loss": 0.6628,
      "step": 480
    },
    {
      "epoch": 0.8009950248756219,
      "grad_norm": 5.999628067016602,
      "learning_rate": 2.0379488128586246e-06,
      "loss": 0.7209,
      "step": 483
    },
    {
      "epoch": 0.8059701492537313,
      "grad_norm": 2.488394260406494,
      "learning_rate": 1.9415120639290086e-06,
      "loss": 0.7918,
      "step": 486
    },
    {
      "epoch": 0.8109452736318408,
      "grad_norm": 3.705122709274292,
      "learning_rate": 1.8471668960045575e-06,
      "loss": 0.5059,
      "step": 489
    },
    {
      "epoch": 0.8159203980099502,
      "grad_norm": 2.4200780391693115,
      "learning_rate": 1.7549377963789994e-06,
      "loss": 0.4715,
      "step": 492
    },
    {
      "epoch": 0.8208955223880597,
      "grad_norm": 2.3100805282592773,
      "learning_rate": 1.6648487031204274e-06,
      "loss": 0.4365,
      "step": 495
    },
    {
      "epoch": 0.8258706467661692,
      "grad_norm": 2.227738380432129,
      "learning_rate": 1.5769229988581514e-06,
      "loss": 0.561,
      "step": 498
    },
    {
      "epoch": 0.8308457711442786,
      "grad_norm": 3.1360716819763184,
      "learning_rate": 1.49118350471375e-06,
      "loss": 0.451,
      "step": 501
    },
    {
      "epoch": 0.835820895522388,
      "grad_norm": 2.5136935710906982,
      "learning_rate": 1.407652474377832e-06,
      "loss": 0.5176,
      "step": 504
    },
    {
      "epoch": 0.8407960199004975,
      "grad_norm": 2.444190502166748,
      "learning_rate": 1.3263515883341071e-06,
      "loss": 0.6207,
      "step": 507
    },
    {
      "epoch": 0.845771144278607,
      "grad_norm": 2.3895535469055176,
      "learning_rate": 1.2473019482322024e-06,
      "loss": 0.483,
      "step": 510
    },
    {
      "epoch": 0.8507462686567164,
      "grad_norm": 1.799409031867981,
      "learning_rate": 1.1705240714107301e-06,
      "loss": 0.6167,
      "step": 513
    },
    {
      "epoch": 0.8557213930348259,
      "grad_norm": 2.734264612197876,
      "learning_rate": 1.0960378855720177e-06,
      "loss": 0.6561,
      "step": 516
    },
    {
      "epoch": 0.8606965174129353,
      "grad_norm": 2.476045846939087,
      "learning_rate": 1.0238627236098619e-06,
      "loss": 0.5724,
      "step": 519
    },
    {
      "epoch": 0.8656716417910447,
      "grad_norm": 2.2838330268859863,
      "learning_rate": 9.54017318591678e-07,
      "loss": 0.5,
      "step": 522
    },
    {
      "epoch": 0.8706467661691543,
      "grad_norm": 1.9036625623703003,
      "learning_rate": 8.865197988963459e-07,
      "loss": 0.5652,
      "step": 525
    },
    {
      "epoch": 0.8756218905472637,
      "grad_norm": 1.4777538776397705,
      "learning_rate": 8.213876835089607e-07,
      "loss": 0.4839,
      "step": 528
    },
    {
      "epoch": 0.8805970149253731,
      "grad_norm": 3.1409499645233154,
      "learning_rate": 7.586378774738012e-07,
      "loss": 0.6694,
      "step": 531
    },
    {
      "epoch": 0.8855721393034826,
      "grad_norm": 2.1010992527008057,
      "learning_rate": 6.98286667506618e-07,
      "loss": 0.4142,
      "step": 534
    },
    {
      "epoch": 0.8905472636815921,
      "grad_norm": 2.318805694580078,
      "learning_rate": 6.403497177674112e-07,
      "loss": 0.5401,
      "step": 537
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 2.299727439880371,
      "learning_rate": 5.84842065794804e-07,
      "loss": 0.4247,
      "step": 540
    },
    {
      "epoch": 0.900497512437811,
      "grad_norm": 2.140589475631714,
      "learning_rate": 5.317781186030413e-07,
      "loss": 0.4357,
      "step": 543
    },
    {
      "epoch": 0.9054726368159204,
      "grad_norm": 2.5464046001434326,
      "learning_rate": 4.811716489426565e-07,
      "loss": 0.4424,
      "step": 546
    },
    {
      "epoch": 0.9104477611940298,
      "grad_norm": 2.849886655807495,
      "learning_rate": 4.3303579172574884e-07,
      "loss": 0.553,
      "step": 549
    },
    {
      "epoch": 0.9154228855721394,
      "grad_norm": 2.4949660301208496,
      "learning_rate": 3.8738304061681107e-07,
      "loss": 0.5074,
      "step": 552
    },
    {
      "epoch": 0.9203980099502488,
      "grad_norm": 9.249605178833008,
      "learning_rate": 3.4422524479001187e-07,
      "loss": 0.597,
      "step": 555
    },
    {
      "epoch": 0.9253731343283582,
      "grad_norm": 2.132213830947876,
      "learning_rate": 3.035736058537231e-07,
      "loss": 0.5222,
      "step": 558
    },
    {
      "epoch": 0.9303482587064676,
      "grad_norm": 1.8414052724838257,
      "learning_rate": 2.6543867494316254e-07,
      "loss": 0.4831,
      "step": 561
    },
    {
      "epoch": 0.9353233830845771,
      "grad_norm": 2.36617374420166,
      "learning_rate": 2.2983034998182997e-07,
      "loss": 0.4386,
      "step": 564
    },
    {
      "epoch": 0.9402985074626866,
      "grad_norm": 1.7980318069458008,
      "learning_rate": 1.967578731125064e-07,
      "loss": 0.4975,
      "step": 567
    },
    {
      "epoch": 0.945273631840796,
      "grad_norm": 3.9248077869415283,
      "learning_rate": 1.662298282984587e-07,
      "loss": 0.4858,
      "step": 570
    },
    {
      "epoch": 0.9502487562189055,
      "grad_norm": 2.481112003326416,
      "learning_rate": 1.3825413909546502e-07,
      "loss": 0.4621,
      "step": 573
    },
    {
      "epoch": 0.9552238805970149,
      "grad_norm": 1.9624525308609009,
      "learning_rate": 1.1283806659525998e-07,
      "loss": 0.4959,
      "step": 576
    },
    {
      "epoch": 0.9601990049751243,
      "grad_norm": 2.5577986240386963,
      "learning_rate": 8.99882075409153e-08,
      "loss": 0.549,
      "step": 579
    },
    {
      "epoch": 0.9651741293532339,
      "grad_norm": 1.7154029607772827,
      "learning_rate": 6.971049261465746e-08,
      "loss": 0.5143,
      "step": 582
    },
    {
      "epoch": 0.9701492537313433,
      "grad_norm": 1.571102261543274,
      "learning_rate": 5.2010184898556585e-08,
      "loss": 0.8452,
      "step": 585
    },
    {
      "epoch": 0.9751243781094527,
      "grad_norm": 4.136885643005371,
      "learning_rate": 3.689187850849374e-08,
      "loss": 0.4834,
      "step": 588
    },
    {
      "epoch": 0.9800995024875622,
      "grad_norm": 2.4916880130767822,
      "learning_rate": 2.4359497401758026e-08,
      "loss": 0.4793,
      "step": 591
    },
    {
      "epoch": 0.9850746268656716,
      "grad_norm": 3.484964370727539,
      "learning_rate": 1.4416294358582383e-08,
      "loss": 0.5816,
      "step": 594
    },
    {
      "epoch": 0.9900497512437811,
      "grad_norm": 2.6594398021698,
      "learning_rate": 7.064850137885604e-09,
      "loss": 0.5484,
      "step": 597
    },
    {
      "epoch": 0.9950248756218906,
      "grad_norm": 2.741891384124756,
      "learning_rate": 2.3070728074381376e-09,
      "loss": 0.5435,
      "step": 600
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.375521421432495,
      "learning_rate": 1.4419724861602923e-10,
      "loss": 0.5456,
      "step": 603
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6278228759765625,
      "eval_runtime": 2.6135,
      "eval_samples_per_second": 2.678,
      "eval_steps_per_second": 2.678,
      "step": 603
    }
  ],
  "logging_steps": 3,
  "max_steps": 603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.628282617285837e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
